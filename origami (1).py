# -*- coding: utf-8 -*-
"""Origami

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ozXBJKhZd3LeVg7hkvja3gdfYRCq9nMw
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Converting Audio to Spectograms

"""

!pip install pydub

pip install --upgrade AudioConverter

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Origami
!ls

import os
file_name = 'my_file.txt'
name, format = os.path.splitext(file_name)
print(name)

from pydub import AudioSegment
import matplotlib.pyplot as plt
from scipy.io import wavfile
from tempfile import mktemp
import os
from pydub import AudioSegment

#audioconvert --verbose/-v convert audio wavaudio --output-format .wav

directory = r'/content/drive/MyDrive/Origami/audio/'
for filename in os.listdir(directory):

  name, format = os.path.splitext(filename)
  
  if filename.endswith(".wav"):
    print(filename)
    FS, data = wavfile.read(directory + filename)  # read wav file
    if len(data.shape) > 1:
      data1 = data[:,0]
    else:
      data1 = data
    plt.specgram(data1, Fs=FS, NFFT=128, noverlap=0)  # plot
    plt.axis('off')
    plt.savefig('/content/drive/MyDrive/Origami/spectograms/'+name+'.jpg', bbox_inches='tight',transparent=True, pad_inches=0)
    plt.show()
  else:
    print(filename)
    mp3_audio = AudioSegment.from_file(directory + filename)
    wname = mktemp('.wav')  # use temporary file
    mp3_audio.export(wname, format="wav")  # convert to wav
    FS, data = wavfile.read(wname)  # read wav file
    if len(data.shape) > 1:
      data1 = data[:,0]
    else:
      data1 = data
    plt.specgram(data1, Fs=FS, NFFT=128, noverlap=0)  # plot
    plt.axis('off')
    plt.savefig('/content/drive/MyDrive/Origami/spectograms/'+name+'.jpg', bbox_inches='tight',transparent=True, pad_inches=0)
    plt.show()

"""### VGG-FACE"""

from keras.models import Model, Sequential
from keras.layers import Activation, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout
from PIL import Image
import numpy as np
from keras.preprocessing.image import load_img, img_to_array
from keras.applications.resnet50 import preprocess_input

model = Sequential()
model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))
model.add(Convolution2D(64, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(Convolution2D(4096, (7, 7), activation='relu'))
model.add(Dropout(0.5))
model.add(Convolution2D(4096, (1, 1), activation='relu', name='fc7'))
model.add(Dropout(0.5))
model.add(Convolution2D(2622, (1, 1)))
model.add(Flatten())
model.add(Activation('softmax'))

from keras.models import model_from_json
model.load_weights('/content/drive/MyDrive/Origami/vgg_face_weights.h5')

def preprocess_image(image_path):
  img = load_img(image_path, target_size=(224, 224))
  img = img_to_array(img)
  img = np.expand_dims(img, axis=0)
  img = preprocess_input(img)
  return img

import os
Image.MAX_IMAGE_PIXELS = None

truth = []
directory = r'/content/drive/MyDrive/Origami/croppedimages'
for filename in os.listdir(directory):
    if filename.endswith(".jpg") or filename.endswith(".png") or filename.endswith(".JPG") or filename.endswith(".jpeg") or filename.endswith(".JPEG"):
        intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer('fc7').output)
        img1_representation = intermediate_layer_model.predict(preprocess_image(directory + '/' + filename))[0,:]
        truth.append(img1_representation)

import numpy as np
np.shape(truth)

np.set_printoptions(threshold=np.inf)
print(truth[2])

"""### Training the Voice Encoder"""

import cv2
import os
import matplotlib.pyplot as plt

# Loading X_train 

def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder,filename))
        if img is not None:
            images.append(img)
    return images
folder="/content/drive/MyDrive/Origami/spectograms"

X_train = load_images_from_folder(folder)
X = np.array(X_train)

plt.imshow(X[0])

#Loading y_train
y_train = np.array(truth)


print(X.shape)
print(y_train.shape)
#shape of spectogram = (217,334,3)

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, ReLU, BatchNormalization, AveragePooling2D

model = Sequential()

model.add(Conv2D(64, kernel_size=(4,4),strides=1,padding="VALID", activation="relu", input_shape=(217,334,3)))
model.add(BatchNormalization(axis=-1))

model.add(Conv2D(64, kernel_size=(4,4),strides=1,padding="VALID", activation="relu"))
model.add(BatchNormalization(axis=-1))

model.add(Conv2D(128, kernel_size=(4,4),strides=1,padding="VALID", activation="relu"))
model.add(BatchNormalization(axis=-1))

model.add(MaxPool2D(pool_size=[2,1], strides=(2,1)))

model.add(Conv2D(128, kernel_size=(4,4),strides=1,padding="VALID", activation="relu"))
model.add(BatchNormalization(axis=-1))

model.add(MaxPool2D(pool_size=[2,1], strides=(2,1)))

model.add(Conv2D(128, kernel_size=(4,4),strides=1,padding="VALID", activation="relu"))
model.add(BatchNormalization(axis=-1))

model.add(MaxPool2D(pool_size=[2,1], strides=(2,1)))

model.add(Conv2D(256, kernel_size=(4,4),strides=1,padding="VALID", activation="relu"))
model.add(BatchNormalization(axis=-1))

model.add(MaxPool2D(pool_size=[2,1], strides=(2,1)))

model.add(Conv2D(512, kernel_size=(4,4),strides=1,padding="VALID", activation="relu"))
model.add(BatchNormalization(axis=-1))
model.add(Conv2D(512, kernel_size=(4,4),strides=1,padding="VALID", activation="relu"))
model.add(BatchNormalization(axis=-1))

model.add(Conv2D(filters=512,kernel_size=(4,4),strides=2,padding="VALID"))

model.add(Flatten())

model.add(Dense(4096, activation = "relu"))

model.add(Dense(4096))

model.summary()

model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy'])
history=model.fit(X,y_train, epochs=100)
plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()

"""### Face Decoder

"""

import cv2
import os
import matplotlib.pyplot as plt
import numpy as np

# Loading images to train the autoencoder

def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder,filename))
        if img is not None:
            images.append(img)
    return images

folder="/content/drive/MyDrive/Origami/images"

Images = load_images_from_folder(folder)
Images = np.array(Images)
plt.axis("off")
plt.imshow(cv2.cvtColor(Images[63], cv2.COLOR_BGR2RGB))
plt.show()

#detect faces from the images and crop them into a common size and save them in a folder
import dlib
from PIL import Image
from skimage import io
import matplotlib.pyplot as plt
import os

def detect_faces(image):
    # Create a face detector
    face_detector = dlib.get_frontal_face_detector()
    # Run detector and get bounding boxes of the faces on image.
    detected_faces = face_detector(image, 1)
    face_frames = [(x.left(), x.top(),
                    x.right(), x.bottom()) for x in detected_faces]
    return face_frames

i=0
# Crop faces and plot
for image in Images:
  detected_face = detect_faces(image)
  for n, face_rect in enumerate(detected_face):
    face = Image.fromarray(image).crop(face_rect)
    new_image = face.resize((400, 400)) 
    i=i+1
    print(i)
    plt.subplot(1, len(detected_face), n+1)
    plt.axis("off")
    plt.savefig('/content/drive/MyDrive/Origami/croppedimages/'+str(i)+'.jpg', bbox_inches='tight',transparent=True, pad_inches=0)
    plt.show()
    plt.imshow(cv2.cvtColor(np.array(new_image), cv2.COLOR_BGR2RGB))

import cv2
import os
import matplotlib.pyplot as plt
import numpy as np

# Loading images to train the autoencoder

def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img = cv2.imread(os.path.join(folder,filename))
        if img is not None:
            images.append(img)
    return images

folder="/content/drive/MyDrive/Origami/croppedimages"

CX = load_images_from_folder(folder)
CX = np.array(CX)
plt.axis("off")
plt.imshow(cv2.cvtColor(CX[60], cv2.COLOR_BGR2RGB))
plt.show()

from keras.layers import Dense, Flatten, Reshape, Input, InputLayer
from keras.models import Sequential, Model

def build_autoencoder(img_shape, code_size):
    # The encoder
    encoder = Sequential()
    encoder.add(InputLayer(img_shape))
    encoder.add(Flatten())
    encoder.add(Dense(code_size))

    # The decoder
    decoder = Sequential()
    decoder.add(InputLayer((code_size,)))
    decoder.add(Dense(np.prod(img_shape))) # np.prod(img_shape) is the same as 32*32*3, it's more generic than saying 3072
    decoder.add(Reshape(img_shape))

    return encoder, decoder

print(CX[2].shape)
print(CX[60].shape)

